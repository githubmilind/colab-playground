{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Implementation of Chain of Verification (CoVe) to reduce hallucination in LLM"
      ],
      "metadata": {
        "id": "JkTuYLwEP-EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai langchain_community"
      ],
      "metadata": {
        "id": "njEGbo6kqo-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "ha9xlqyTqC-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "JF9bRjxR33uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "vydfitto359h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AIBAwkph4FIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "llm = ChatOpenAI(model_name=\"gpt-5-nano\")\n",
        "\n",
        "query = \"List 5 UK politicians born in London\""
      ],
      "metadata": {
        "id": "Te69kobGqi_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate baseline response"
      ],
      "metadata": {
        "id": "NsYAgqDf4eSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_variables = [\"query\"]\n",
        "base_response_output_key = \"base_response\"\n",
        "base_response_template = \"\"\"Question: {query} Answer:\"\"\"\n",
        "base_response_prompt_template = PromptTemplate(\n",
        "    input_variables=input_variables, template=base_response_template\n",
        ")\n",
        "base_reponse_chain = LLMChain(\n",
        "    llm=llm, prompt=base_response_prompt_template,\n",
        "    output_key=base_response_output_key\n",
        ")"
      ],
      "metadata": {
        "id": "9t5utonC4jsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan verification"
      ],
      "metadata": {
        "id": "UXXdI9C85X7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plan_verifications_template = \"\"\"\n",
        "Given the below Question and answer, generate a series of verification questions that test the factual claims in the original baseline response.\n",
        "For example if part of a longform model response contains the statement “The Mexican–American War\n",
        "was an armed conflict between the United States and Mexico from 1846 to 1848”, then one possible\n",
        "verification question to check those dates could be “When did the Mexican American war start and\n",
        "end?”\n",
        "\n",
        "Question: {query}\n",
        "Answer: {base_response}\n",
        "\n",
        "<fact in passage>, <verification question, generated by combining the query and the fact>\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "class PlanVerificationsOutput(BaseModel):\n",
        "    query: str = Field(description=\"The user's query\")\n",
        "    base_response: str = Field(description=\"The response to the user's query\")\n",
        "    facts_and_verification_questions: dict[str, str] = Field(\n",
        "        description=\"Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)\"\n",
        "    )\n",
        "\n",
        "plan_verifications_output_parser = PydanticOutputParser(\n",
        "    pydantic_object=PlanVerificationsOutput\n",
        ")\n",
        "\n",
        "plan_verifications_prompt_template = PromptTemplate(\n",
        "    input_variables=input_variables + [base_response_output_key],\n",
        "    template=plan_verifications_template,\n",
        "    partial_variables={\n",
        "        \"format_instructions\": plan_verifications_output_parser.get_format_instructions()\n",
        "    },\n",
        ")\n",
        "\n",
        "plan_verifications_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=plan_verifications_prompt_template,\n",
        "    output_key=\"output\",\n",
        "    output_parser=plan_verifications_output_parser,\n",
        ")"
      ],
      "metadata": {
        "id": "uANfL2Ii5l_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 2 chains (base_response_chain and plan_verifications_chain can then be run sequentially\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "spNljJRr-Gd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_and_plan_verification = SequentialChain(\n",
        "    chains=[base_reponse_chain, plan_verifications_chain],\n",
        "    input_variables=[\"query\"],\n",
        "    output_variables=[\"output\"],\n",
        "    verbose=True\n",
        ")\n",
        "intermediate_result = answer_and_plan_verification.run(query)"
      ],
      "metadata": {
        "id": "XJDKs7RM-VUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4d76c10"
      },
      "source": [
        "Generate verification chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40574555"
      },
      "source": [
        "# Define Pydantic model for verification output\n",
        "class VerificationOutput(BaseModel):\n",
        "    fact: str = Field(description=\"The fact being verified\")\n",
        "    verification_question: str = Field(description=\"The question used to verify the fact\")\n",
        "    answer: str = Field(description=\"The answer to the verification question from the LLM\")\n",
        "    is_consistent: bool = Field(description=\"Whether the answer is consistent with the original fact (True/False)\")\n",
        "    inconsistency_reason: str | None = Field(\n",
        "        description=\"Reason for inconsistency, if applicable\"\n",
        "    )\n",
        "\n",
        "# Create PydanticOutputParser for verification output\n",
        "verification_output_parser = PydanticOutputParser(pydantic_object=VerificationOutput)\n",
        "\n",
        "# Define verification template\n",
        "verification_template = \"\"\"\n",
        "Given the context and the fact, answer the verification question and indicate if the answer is consistent with the original fact.\n",
        "\n",
        "Context: {base_response}\n",
        "Fact: {fact}\n",
        "Verification Question: {verification_question}\n",
        "\n",
        "Answer: {answer}\n",
        "Is Consistent: {is_consistent}\n",
        "Reason for inconsistency: {inconsistency_reason}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "# Create PromptTemplate for verification\n",
        "verification_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"base_response\", \"fact\", \"verification_question\"],\n",
        "    template=verification_template,\n",
        "    partial_variables={\n",
        "        \"format_instructions\": verification_output_parser.get_format_instructions()\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create LLMChain for verification\n",
        "verification_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=verification_prompt_template,\n",
        "    output_key=\"verification_output\",\n",
        "    output_parser=verification_output_parser,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}